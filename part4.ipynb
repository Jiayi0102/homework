{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Core libraries for data manipulation, numerical operations, and file handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Libraries for data acquisition and extraction\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# Libraries for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Library for statistical modeling (regression)\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm # Used for calculating SE of difference in means later\n",
    "\n",
    "# --- Notebook Setup ---\n",
    "# Ensure plots appear directly within the notebook output\n",
    "%matplotlib inline\n",
    "# Set a visually appealing default style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "# Set a default color palette for consistency\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "print(\"Required libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: did_krueger_py\n",
      "Downloading data from http://davidcard.berkeley.edu/data_sets/njmin.zip...\n",
      "File downloaded successfully to did_krueger_py\\wages.zip\n",
      "Extracting contents of did_krueger_py\\wages.zip...\n",
      "Files extracted successfully to did_krueger_py\n"
     ]
    }
   ],
   "source": [
    "# --- File Paths and Directory Setup ---\n",
    "# Define the directory name where data will be stored\n",
    "direct = \"did_krueger_py\"\n",
    "# URL for the dataset\n",
    "URL = \"http://davidcard.berkeley.edu/data_sets/njmin.zip\"\n",
    "# Define full paths for the zip file and its expected contents\n",
    "zip_destfile = os.path.join(direct, \"wages.zip\")\n",
    "codebook_file = os.path.join(direct, \"codebook\")\n",
    "data_file = os.path.join(direct, \"public.dat\")\n",
    "\n",
    "# --- Directory Creation ---\n",
    "# Create the target directory if it doesn't already exist\n",
    "if not os.path.exists(direct):\n",
    "    os.makedirs(direct)\n",
    "    print(f\"Directory created: {direct}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {direct}\")\n",
    "\n",
    "# --- Data Download ---\n",
    "# Check if the zip file already exists before downloading\n",
    "if not os.path.exists(zip_destfile):\n",
    "    print(f\"Downloading data from {URL}...\")\n",
    "    try:\n",
    "        # Send a request to the URL\n",
    "        response = requests.get(URL)\n",
    "        # Raise an exception if the download fails (e.g., 404 Not Found)\n",
    "        response.raise_for_status()\n",
    "        # Write the downloaded content to the zip file\n",
    "        with open(zip_destfile, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File downloaded successfully to {zip_destfile}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle potential download errors\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "else:\n",
    "    # Skip download if file exists\n",
    "    print(f\"Zip file {zip_destfile} already exists. Skipping download.\")\n",
    "\n",
    "# --- Data Extraction ---\n",
    "# Check if the main data file already exists before extracting\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"Extracting contents of {zip_destfile}...\")\n",
    "    try:\n",
    "        # Open the zip file in read mode\n",
    "        with zipfile.ZipFile(zip_destfile, 'r') as zf:\n",
    "            # Extract all contents into the target directory\n",
    "            zf.extractall(path=direct)\n",
    "        print(f\"Files extracted successfully to {direct}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        # Handle corrupted zip file errors\n",
    "        print(f\"Error: Downloaded file {zip_destfile} is not a valid zip file or is corrupted.\")\n",
    "    except FileNotFoundError:\n",
    "        # Handle case where zip file wasn't downloaded\n",
    "        print(f\"Error: Zip file {zip_destfile} not found. Cannot extract.\")\n",
    "    except Exception as e:\n",
    "        # Handle other potential extraction errors\n",
    "        print(f\"An error occurred during extraction: {e}\")\n",
    "else:\n",
    "     # Skip extraction if data file exists\n",
    "     print(f\"Data file {data_file} already exists. Skipping extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook read successfully.\n",
      "Extracted 46 variable names.\n",
      "First 10 variable names: ['sheet', 'chain', 'co_owned', 'state', 'southj', 'centralj', 'northj', 'pa1', 'pa2', 'shore']\n",
      "\n",
      "Raw data loaded successfully with shape: (410, 46)\n",
      "Column names assigned successfully.\n",
      "\n",
      "Data types converted: 'sheet' to string, others to numeric (with errors as NaN).\n",
      "\n",
      "Dataset Structure (Info):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 410 entries, 0 to 409\n",
      "Data columns (total 46 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   sheet     410 non-null    object \n",
      " 1   chain     410 non-null    int64  \n",
      " 2   co_owned  410 non-null    int64  \n",
      " 3   state     410 non-null    int64  \n",
      " 4   southj    410 non-null    int64  \n",
      " 5   centralj  410 non-null    int64  \n",
      " 6   northj    410 non-null    int64  \n",
      " 7   pa1       410 non-null    int64  \n",
      " 8   pa2       410 non-null    int64  \n",
      " 9   shore     410 non-null    int64  \n",
      " 10  ncalls    410 non-null    int64  \n",
      " 11  empft     404 non-null    float64\n",
      " 12  emppt     406 non-null    float64\n",
      " 13  nmgrs     404 non-null    float64\n",
      " 14  wage_st   390 non-null    float64\n",
      " 15  inctime   379 non-null    float64\n",
      " 16  firstinc  367 non-null    float64\n",
      " 17  bonus     410 non-null    int64  \n",
      " 18  pctaff    366 non-null    float64\n",
      " 19  meals     410 non-null    int64  \n",
      " 20  open      410 non-null    float64\n",
      " 21  hrsopen   410 non-null    float64\n",
      " 22  psoda     402 non-null    float64\n",
      " 23  pfry      393 non-null    float64\n",
      " 24  pentree   398 non-null    float64\n",
      " 25  nregs     404 non-null    float64\n",
      " 26  nregs11   398 non-null    float64\n",
      " 27  type2     410 non-null    int64  \n",
      " 28  status2   410 non-null    int64  \n",
      " 29  date2     410 non-null    int64  \n",
      " 30  ncalls2   161 non-null    float64\n",
      " 31  empft2    398 non-null    float64\n",
      " 32  emppt2    400 non-null    float64\n",
      " 33  nmgrs2    404 non-null    float64\n",
      " 34  wage_st2  389 non-null    float64\n",
      " 35  inctime2  344 non-null    float64\n",
      " 36  firstin2  330 non-null    float64\n",
      " 37  special2  392 non-null    float64\n",
      " 38  meals2    399 non-null    float64\n",
      " 39  open2r    399 non-null    float64\n",
      " 40  hrsopen2  399 non-null    float64\n",
      " 41  psoda2    388 non-null    float64\n",
      " 42  pfry2     382 non-null    float64\n",
      " 43  pentree2  386 non-null    float64\n",
      " 44  nregs2    388 non-null    float64\n",
      " 45  nregs112  383 non-null    float64\n",
      "dtypes: float64(30), int64(15), object(1)\n",
      "memory usage: 147.5+ KB\n",
      "\n",
      "First 5 Rows of Loaded Dataset (Head):\n",
      "  sheet  chain  co_owned  state  southj  centralj  northj  pa1  pa2  shore  \\\n",
      "0    46      1         0      0       0         0       0    1    0      0   \n",
      "1    49      2         0      0       0         0       0    1    0      0   \n",
      "2   506      2         1      0       0         0       0    1    0      0   \n",
      "3    56      4         1      0       0         0       0    1    0      0   \n",
      "4    61      4         1      0       0         0       0    1    0      0   \n",
      "\n",
      "   ...  firstin2  special2  meals2  open2r  hrsopen2  psoda2  pfry2  pentree2  \\\n",
      "0  ...      0.08       1.0     2.0     6.5      16.5    1.03    NaN      0.94   \n",
      "1  ...      0.05       0.0     2.0    10.0      13.0    1.01   0.89      2.35   \n",
      "2  ...      0.25       NaN     1.0    11.0      11.0    0.95   0.74      2.33   \n",
      "3  ...      0.15       0.0     2.0    10.0      12.0    0.92   0.79      0.87   \n",
      "4  ...      0.15       0.0     2.0    10.0      12.0    1.01   0.84      0.95   \n",
      "\n",
      "   nregs2  nregs112  \n",
      "0     4.0       4.0  \n",
      "1     4.0       4.0  \n",
      "2     4.0       3.0  \n",
      "3     2.0       2.0  \n",
      "4     2.0       2.0  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2848\\793259616.py:47: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  dataset_raw = pd.read_csv(data_file, delim_whitespace=True, header=None, na_values=[\".\"])\n"
     ]
    }
   ],
   "source": [
    "# --- Codebook Parsing for Variable Names ---\n",
    "variable_names = [] # Initialize an empty list to store names\n",
    "try:\n",
    "    # Open the codebook file. 'latin1' encoding is often needed for older text files\n",
    "    # that might contain special characters not handled by standard 'utf-8'.\n",
    "    with open(codebook_file, 'r', encoding='latin1') as f:\n",
    "        codebook_lines = f.readlines()\n",
    "    print(\"Codebook read successfully.\")\n",
    "\n",
    "    # Extract variable names based on observed structure in codebook lines 8-59\n",
    "    # (Python index 7 to 58), skipping specified intermediate description lines.\n",
    "    # Indices to skip within the 7:58 slice (0-based): 4, 5, 12, 13, 31, 32\n",
    "    indices_to_remove_within_slice = {4, 5, 12, 13, 31, 32}\n",
    "    codebook_relevant_lines = codebook_lines[7:59] # Slice lines 8-59\n",
    "\n",
    "    for i, line in enumerate(codebook_relevant_lines):\n",
    "        # Skip the lines specified\n",
    "        if i not in indices_to_remove_within_slice:\n",
    "            # Extract name from the first 13 characters, remove whitespace, convert to lowercase\n",
    "            var_name = line[:13].strip().lower()\n",
    "            # Ensure we don't add empty strings if a line was improperly formatted\n",
    "            if var_name:\n",
    "                 variable_names.append(var_name)\n",
    "\n",
    "    print(f\"Extracted {len(variable_names)} variable names.\")\n",
    "    if variable_names:\n",
    "        print(\"First 10 variable names:\", variable_names[:10])\n",
    "    else:\n",
    "        # This would indicate a problem with the parsing logic or codebook format\n",
    "        print(\"Warning: No variable names were extracted. Check codebook parsing logic.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Codebook file not found at {codebook_file}. Cannot extract variable names.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred reading or parsing the codebook: {e}\")\n",
    "\n",
    "# --- Data Loading and Initial Cleaning ---\n",
    "dataset = pd.DataFrame() # Initialize an empty DataFrame in case loading fails\n",
    "\n",
    "# Proceed only if variable names were successfully extracted\n",
    "if variable_names:\n",
    "    try:\n",
    "        # Load the dataset:\n",
    "        # - delim_whitespace=True handles space-separated values\n",
    "        # - header=None specifies that the file has no header row\n",
    "        # - na_values=[\".\"] tells pandas to treat '.' characters as missing (NaN)\n",
    "        dataset_raw = pd.read_csv(data_file, delim_whitespace=True, header=None, na_values=[\".\"])\n",
    "        print(f\"\\nRaw data loaded successfully with shape: {dataset_raw.shape}\")\n",
    "\n",
    "        # Verify column count consistency and handle potential extra empty columns\n",
    "        expected_cols = len(variable_names)\n",
    "        actual_cols = dataset_raw.shape[1]\n",
    "\n",
    "        if actual_cols > expected_cols:\n",
    "             print(f\"Data has {actual_cols} columns, but {expected_cols} names were extracted.\")\n",
    "             # Check if the extra columns are completely empty or just NaN\n",
    "             extra_cols_data = dataset_raw.iloc[:, expected_cols:]\n",
    "             if extra_cols_data.isnull().all().all():\n",
    "                  print(f\"Dropping {actual_cols - expected_cols} trailing empty/NA column(s).\")\n",
    "                  dataset_raw = dataset_raw.iloc[:, :expected_cols]\n",
    "             else:\n",
    "                  # This case is less likely but handled defensively\n",
    "                  print(f\"Warning: Trailing columns contain data, but are being dropped to match extracted variable names.\")\n",
    "                  dataset_raw = dataset_raw.iloc[:, :expected_cols]\n",
    "        elif actual_cols < expected_cols:\n",
    "             print(f\"Error: Data has {actual_cols} columns, but {expected_cols} names were extracted. Aborting.\")\n",
    "             variable_names = [] # Prevent further processing\n",
    "\n",
    "        # Assign column names if counts match\n",
    "        if dataset_raw.shape[1] == expected_cols:\n",
    "            dataset_raw.columns = variable_names\n",
    "            print(\"Column names assigned successfully.\")\n",
    "\n",
    "            # --- Data Type Conversion ---\n",
    "            # Store the 'sheet' column (store ID) separately before converting others\n",
    "            # Convert 'sheet' to string type for identification purposes\n",
    "            sheet_col = dataset_raw['sheet'].astype(str)\n",
    "\n",
    "            # Convert all other columns to numeric type.\n",
    "            # 'errors='coerce'' will turn any value that cannot be converted into NaN (Not a Number).\n",
    "            dataset = dataset_raw.drop(columns=['sheet']).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            # Re-insert the 'sheet' column at the beginning of the DataFrame\n",
    "            dataset.insert(0, 'sheet', sheet_col)\n",
    "\n",
    "            print(\"\\nData types converted: 'sheet' to string, others to numeric (with errors as NaN).\")\n",
    "            print(\"\\nDataset Structure (Info):\")\n",
    "            dataset.info()\n",
    "            print(\"\\nFirst 5 Rows of Loaded Dataset (Head):\")\n",
    "            print(dataset.head())\n",
    "\n",
    "        else:\n",
    "            # Handle the case where column count mismatch persists\n",
    "            print(f\"Error: Column count mismatch after attempting cleanup ({dataset_raw.shape[1]} vs {expected_cols}). Cannot proceed.\")\n",
    "            dataset = pd.DataFrame() # Ensure dataset is empty\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Data file not found at {data_file}.\")\n",
    "        dataset = pd.DataFrame() # Ensure dataset is empty\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading or processing the data file: {e}\")\n",
    "        dataset = pd.DataFrame() # Ensure dataset is empty\n",
    "else:\n",
    "    print(\"\\nCannot load data because variable names were not extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTE 'before' and 'after' variables calculated.\n",
      "Displaying relevant columns from the first 5 rows:\n",
      "  sheet state  empft  emppt  nmgrs  fte_before  empft2  emppt2  nmgrs2  \\\n",
      "0    46     0   30.0   15.0    3.0       40.50     3.5    35.0     3.0   \n",
      "1    49     0    6.5    6.5    4.0       13.75     0.0    15.0     4.0   \n",
      "2   506     0    3.0    7.0    2.0        8.50     3.0     7.0     4.0   \n",
      "3    56     0   20.0   20.0    4.0       34.00     0.0    36.0     2.0   \n",
      "4    61     0    6.0   26.0    5.0       24.00    28.0     3.0     6.0   \n",
      "\n",
      "   fte_after  \n",
      "0       24.0  \n",
      "1       11.5  \n",
      "2       10.5  \n",
      "3       20.0  \n",
      "4       35.5  \n",
      "\n",
      "Unique values found in 'status2' column: [1 3 4 2 0 5]\n",
      "Value counts for 'status2':\n",
      "status2\n",
      "1    399\n",
      "3      6\n",
      "2      2\n",
      "4      1\n",
      "0      1\n",
      "5      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if the dataset was loaded successfully\n",
    "if not dataset.empty:\n",
    "    # --- Select Relevant Columns ---\n",
    "    # Create a new DataFrame 'data1' with variables needed for analysis.\n",
    "    # Using .copy() prevents SettingWithCopyWarning later.\n",
    "    relevant_columns = [\n",
    "        'sheet', # Store identifier\n",
    "        'state', # 0=PA, 1=NJ (Treatment indicator)\n",
    "        'chain', # Restaurant chain (potential control variable)\n",
    "        'co_owned', # Company owned vs. franchisee (potential control)\n",
    "        # Location dummies (could be used as controls, not used in basic model)\n",
    "        'southj', 'centralj', 'northj', 'pa1', 'pa2',\n",
    "        # Wave 1 (Before) variables\n",
    "        'empft', 'emppt', 'nmgrs', 'wage_st', 'hrsopen',\n",
    "        # Wave 2 (After) variables\n",
    "        'empft2', 'emppt2', 'nmgrs2', 'wage_st2', 'hrsopen2',\n",
    "        # Status of second interview (important for sample definition)\n",
    "        'status2'\n",
    "    ]\n",
    "    data1 = dataset[relevant_columns].copy()\n",
    "\n",
    "    # --- Clean State Indicator ---\n",
    "    # Ensure 'state' is represented as strings '0' and '1' for clarity in grouping/modeling\n",
    "    # First, handle potential NaN values if any exist, then convert type\n",
    "    data1['state'] = data1['state'].dropna().astype(int).astype(str)\n",
    "\n",
    "    # --- Calculate FTE Variables ---\n",
    "    # FTE Before = FullTime_1 + Managers_1 + 0.5 * PartTime_1\n",
    "    data1['fte_before'] = data1['empft'] + data1['nmgrs'] + data1['emppt'] * 0.5\n",
    "    # FTE After = FullTime_2 + Managers_2 + 0.5 * PartTime_2\n",
    "    data1['fte_after'] = data1['empft2'] + data1['nmgrs2'] + data1['emppt2'] * 0.5\n",
    "\n",
    "    print(\"FTE 'before' and 'after' variables calculated.\")\n",
    "    print(\"Displaying relevant columns from the first 5 rows:\")\n",
    "    print(data1[['sheet', 'state', 'empft', 'emppt', 'nmgrs', 'fte_before',\n",
    "                 'empft2', 'emppt2', 'nmgrs2', 'fte_after']].head())\n",
    "\n",
    "    # --- Examine Status Variable ---\n",
    "    # Understand the different outcomes of the second interview wave\n",
    "    # Codes: 0=refused, 1=answered, 2=closed renovations, 3=closed permanently,\n",
    "    # 4=closed highway construction, 5=closed mall fire\n",
    "    print(\"\\nUnique values found in 'status2' column:\", data1['status2'].unique())\n",
    "    print(\"Value counts for 'status2':\")\n",
    "    print(data1['status2'].value_counts())\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Feature Engineering step as the initial dataset is empty or failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics for Key Variables:\n",
      "            Valid N   Mean  Std. Dev.   Min    Max\n",
      "chain           410   2.12       1.11  1.00   4.00\n",
      "co_owned        410   0.34       0.48  0.00   1.00\n",
      "empft           404   8.20       8.62  0.00  60.00\n",
      "emppt           406  18.83      10.08  0.00  60.00\n",
      "nmgrs           404   3.42       1.02  1.00  10.00\n",
      "fte_before      398  21.00       9.75  5.00  85.00\n",
      "wage_st         390   4.62       0.35  4.25   5.75\n",
      "hrsopen         410  14.44       2.81  7.00  24.00\n",
      "empft2          398   8.28       7.97  0.00  40.00\n",
      "emppt2          400  18.68      10.70  0.00  60.00\n",
      "nmgrs2          404   3.48       1.14  0.00   8.00\n",
      "fte_after       396  21.05       9.09  0.00  60.50\n",
      "wage_st2        389   5.00       0.25  4.25   6.25\n",
      "hrsopen2        399  14.47       2.75  8.00  24.00\n",
      "status2         410   1.05       0.35  0.00   5.00\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if data1 was created successfully\n",
    "if 'data1' in locals() and not data1.empty:\n",
    "    # --- Define Variables for Summary ---\n",
    "    # List of columns for which we want descriptive statistics\n",
    "    cols_for_desc_stats = [\n",
    "        'state', 'chain', 'co_owned',\n",
    "        'empft', 'emppt', 'nmgrs', 'fte_before', 'wage_st', 'hrsopen',\n",
    "        'empft2', 'emppt2', 'nmgrs2', 'fte_after', 'wage_st2', 'hrsopen2',\n",
    "        'status2'\n",
    "    ]\n",
    "    # Ensure only columns present in data1 are included\n",
    "    cols_for_desc_stats = [col for col in cols_for_desc_stats if col in data1.columns]\n",
    "\n",
    "    # --- Calculate Descriptive Statistics ---\n",
    "    # Use pandas .describe() method for numerical summaries\n",
    "    # .transpose() makes variables the rows for easier reading\n",
    "    desc_stats = data1[cols_for_desc_stats].describe().transpose()\n",
    "\n",
    "    # --- Format the Output Table ---\n",
    "    # Select only the desired statistics\n",
    "    desc_stats_formatted = desc_stats[['count', 'mean', 'std', 'min', 'max']].copy()\n",
    "    # Rename columns for better readability\n",
    "    desc_stats_formatted.rename(columns={\n",
    "        'count': 'Valid N', # Number of non-missing observations\n",
    "        'mean': 'Mean',\n",
    "        'std': 'Std. Dev.',\n",
    "        'min': 'Min',\n",
    "        'max': 'Max'\n",
    "    }, inplace=True)\n",
    "    # Convert 'Valid N' to integer type\n",
    "    desc_stats_formatted['Valid N'] = desc_stats_formatted['Valid N'].astype(int)\n",
    "\n",
    "    print(\"\\nDescriptive Statistics for Key Variables:\")\n",
    "    # Display the formatted table, showing precision to 2 decimal places\n",
    "    print(desc_stats_formatted.round(2))\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Descriptive Statistics as the 'data1' DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics (Total Sample):\n",
      "            Valid N       Mean  Std. dev.   Min    Max\n",
      "co_owned        410   0.343902   0.475589  0.00   1.00\n",
      "southj          410   0.226829   0.419293  0.00   1.00\n",
      "centralj        410   0.153659   0.361062  0.00   1.00\n",
      "northj          410   0.426829   0.495221  0.00   1.00\n",
      "pa1             410   0.087805   0.283357  0.00   1.00\n",
      "pa2             410   0.104878   0.306771  0.00   1.00\n",
      "wage_st         390   4.615641   0.347015  4.25   5.75\n",
      "wage_st2        389   4.996272   0.253190  4.25   6.25\n",
      "hrsopen         410  14.439024   2.809987  7.00  24.00\n",
      "hrsopen2        399  14.465539   2.752495  8.00  24.00\n",
      "empft           404   8.202970   8.619523  0.00  60.00\n",
      "emppt           406  18.831281  10.081727  0.00  60.00\n",
      "nmgrs           404   3.420297   1.018408  1.00  10.00\n",
      "empft2          398   8.275126   7.970763  0.00  40.00\n",
      "emppt2          400  18.677500  10.699635  0.00  60.00\n",
      "nmgrs2          404   3.483911   1.139898  0.00   8.00\n",
      "chain           410   2.117073   1.110497  1.00   4.00\n",
      "status2         410   1.048780   0.353205  0.00   5.00\n",
      "fte_before      398  20.998869   9.749805  5.00  85.00\n",
      "fte_after       396  21.054293   9.094453  0.00  60.50\n"
     ]
    }
   ],
   "source": [
    "if not data1.empty:\n",
    "    # Select columns for descriptive statistics\n",
    "    cols_for_desc = [\n",
    "        'co_owned', 'southj', 'centralj', 'northj', 'pa1', 'pa2',\n",
    "        'wage_st', 'wage_st2', 'hrsopen', 'hrsopen2',\n",
    "        'empft', 'emppt', 'nmgrs', # Before counts\n",
    "        'empft2','emppt2', 'nmgrs2',# After counts\n",
    "        'chain', 'status2',\n",
    "        'fte_before', 'fte_after' # Calculated FTE\n",
    "    ]\n",
    "\n",
    "    # Use pandas describe and select relevant stats\n",
    "    desc_stats = data1[cols_for_desc].describe().transpose()\n",
    "\n",
    "    # Select and rename columns to closely match R output\n",
    "    desc_stats_formatted = desc_stats[['count', 'mean', 'std', 'min', 'max']].copy()\n",
    "    desc_stats_formatted.rename(columns={\n",
    "        'count': 'Valid N',\n",
    "        'mean': 'Mean',\n",
    "        'std': 'Std. dev.',\n",
    "        'min': 'Min',\n",
    "        'max': 'Max'\n",
    "    }, inplace=True)\n",
    "\n",
    "    # Ensure 'Valid N' is integer\n",
    "    desc_stats_formatted['Valid N'] = desc_stats_formatted['Valid N'].astype(int)\n",
    "\n",
    "    print(\"\\nDescriptive Statistics (Total Sample):\")\n",
    "    print(desc_stats_formatted)\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Descriptive Statistics as data1 is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Difference-in-Differences using Group Means (Table 2 Replication) ---\n",
      "\n",
      "Means based on all available data per wave:\n",
      "  state_name  mean_before  se_mean_before  n_before  mean_after  \\\n",
      "0         PA       23.331           1.351        77      21.166   \n",
      "1         NJ       20.439           0.508       321      21.027   \n",
      "\n",
      "   se_mean_after  n_after  \n",
      "0          0.943       77  \n",
      "1          0.520      319  \n",
      "\n",
      "Number of stores in balanced panel: 384\n",
      "\n",
      "Change in means based on balanced panel:\n",
      "  state_name  change_mean_fte_balanced  se_change_balanced  n_balanced\n",
      "0         PA                    -2.283               1.253          75\n",
      "1         NJ                     0.467               0.481         309\n",
      "\n",
      "--- Summary Table: FTE Employment Before and After ---\n",
      "                              Variable      PA      NJ  Difference (NJ - PA)\n",
      "0                    Mean FTE (Before)  23.331  20.439                -2.892\n",
      "1                          SE (Before)   1.351   0.508                -0.843\n",
      "2                     Mean FTE (After)  21.166  21.027                -0.138\n",
      "3                           SE (After)   0.943   0.520                -0.423\n",
      "4        Change in Mean FTE (Balanced)  -2.283   0.467                 2.750\n",
      "5              SE of Change (Balanced)   1.253   0.481                -0.772\n",
      "6        DiD Estimate (Balanced Means)     NaN     NaN                 2.750\n",
      "7  SE of DiD Estimate (Balanced Means)     NaN     NaN                 1.342\n",
      "\n",
      "Sample Sizes (Balanced Panel Used for Change Calculation):\n",
      "            n_balanced\n",
      "state_name            \n",
      "PA                  75\n",
      "NJ                 309\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if data1 is available\n",
    "if 'data1' in locals() and not data1.empty:\n",
    "    print(\"\\n--- Calculating Difference-in-Differences using Group Means (Table 2 Replication) ---\")\n",
    "\n",
    "    # --- Calculations using ALL available data for initial means ---\n",
    "    # Group by state and calculate mean, variance, and count for FTE before/after\n",
    "    summary_stats_all = data1.groupby('state').agg(\n",
    "        mean_before=('fte_before', 'mean'),\n",
    "        mean_after=('fte_after', 'mean'),\n",
    "        var_before=('fte_before', 'var'),\n",
    "        var_after=('fte_after', 'var'),\n",
    "        n_before=('fte_before', 'count'),\n",
    "        n_after=('fte_after', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate Standard Errors (SE) for the means using all available data\n",
    "    summary_stats_all['se_mean_before'] = np.sqrt(summary_stats_all['var_before'] / summary_stats_all['n_before'])\n",
    "    summary_stats_all['se_mean_after'] = np.sqrt(summary_stats_all['var_after'] / summary_stats_all['n_after'])\n",
    "    summary_stats_all['state_name'] = summary_stats_all['state'].map({'0': 'PA', '1': 'NJ'})\n",
    "    print(\"\\nMeans based on all available data per wave:\")\n",
    "    print(summary_stats_all[['state_name', 'mean_before', 'se_mean_before', 'n_before', 'mean_after', 'se_mean_after', 'n_after']].round(3))\n",
    "\n",
    "\n",
    "    # --- Calculations using the BALANCED PANEL for changes ---\n",
    "    # Create balanced panel: Stores with non-missing FTE in BOTH waves\n",
    "    balanced_panel = data1.dropna(subset=['fte_before', 'fte_after']).copy()\n",
    "    print(f\"\\nNumber of stores in balanced panel: {len(balanced_panel)}\")\n",
    "\n",
    "    # Calculate stats using only the balanced panel\n",
    "    balanced_stats = balanced_panel.groupby('state').agg(\n",
    "        mean_before_bal=('fte_before', 'mean'),\n",
    "        mean_after_bal=('fte_after', 'mean'),\n",
    "        var_before_bal=('fte_before', 'var'),\n",
    "        var_after_bal=('fte_after', 'var'),\n",
    "        n_balanced=('sheet', 'count') # Count stores in balanced sample\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the CHANGE in means for the balanced panel\n",
    "    balanced_stats['change_mean_fte_balanced'] = balanced_stats['mean_after_bal'] - balanced_stats['mean_before_bal']\n",
    "\n",
    "    # Calculate covariance between FTE_before and FTE_after within the balanced panel for SE calculation\n",
    "    cov_pa = balanced_panel[balanced_panel['state']=='0'][['fte_before', 'fte_after']].cov().iloc[0,1]\n",
    "    cov_nj = balanced_panel[balanced_panel['state']=='1'][['fte_before', 'fte_after']].cov().iloc[0,1]\n",
    "\n",
    "    # Calculate Variance of the CHANGE in FTE for the balanced panel\n",
    "    # Var(After - Before) = Var(After) + Var(Before) - 2*Cov(After, Before)\n",
    "    balanced_stats['var_change_bal'] = (balanced_stats['var_after_bal'] + balanced_stats['var_before_bal'] - 2 * balanced_stats['state'].map({'0': cov_pa, '1': cov_nj}))\n",
    "\n",
    "    # Calculate Standard Error (SE) of the CHANGE in mean FTE for the balanced panel\n",
    "    # SE(change) = sqrt( Var(change) / n )\n",
    "    balanced_stats['se_change_balanced'] = np.sqrt(balanced_stats['var_change_bal'] / balanced_stats['n_balanced'])\n",
    "    balanced_stats['state_name'] = balanced_stats['state'].map({'0': 'PA', '1': 'NJ'})\n",
    "\n",
    "    print(\"\\nChange in means based on balanced panel:\")\n",
    "    print(balanced_stats[['state_name', 'change_mean_fte_balanced', 'se_change_balanced', 'n_balanced']].round(3))\n",
    "\n",
    "    # --- Assemble the Final Table ---\n",
    "    # Use means from all data, but change from balanced data, mirroring paper's Table 2 structure\n",
    "    final_table_structure = {\n",
    "        'Variable': ['Mean FTE (Before)', 'SE (Before)', 'Mean FTE (After)', 'SE (After)', 'Change in Mean FTE (Balanced)', 'SE of Change (Balanced)'],\n",
    "        'PA': [\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='0', 'mean_before'].iloc[0],\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='0', 'se_mean_before'].iloc[0],\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='0', 'mean_after'].iloc[0],\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='0', 'se_mean_after'].iloc[0],\n",
    "            balanced_stats.loc[balanced_stats['state']=='0', 'change_mean_fte_balanced'].iloc[0],\n",
    "            balanced_stats.loc[balanced_stats['state']=='0', 'se_change_balanced'].iloc[0]\n",
    "        ],\n",
    "        'NJ': [\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='1', 'mean_before'].iloc[0],\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='1', 'se_mean_before'].iloc[0],\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='1', 'mean_after'].iloc[0],\n",
    "            summary_stats_all.loc[summary_stats_all['state']=='1', 'se_mean_after'].iloc[0],\n",
    "            balanced_stats.loc[balanced_stats['state']=='1', 'change_mean_fte_balanced'].iloc[0],\n",
    "            balanced_stats.loc[balanced_stats['state']=='1', 'se_change_balanced'].iloc[0]\n",
    "        ]\n",
    "    }\n",
    "    final_table_df = pd.DataFrame(final_table_structure)\n",
    "    final_table_df['Difference (NJ - PA)'] = final_table_df['NJ'] - final_table_df['PA']\n",
    "\n",
    "    # Calculate the DiD estimate and its SE from the balanced means changes\n",
    "    did_estimate_means = final_table_df.loc[final_table_df['Variable'] == 'Change in Mean FTE (Balanced)', 'Difference (NJ - PA)'].iloc[0]\n",
    "    se_change_pa = final_table_df.loc[final_table_df['Variable'] == 'SE of Change (Balanced)', 'PA'].iloc[0]\n",
    "    se_change_nj = final_table_df.loc[final_table_df['Variable'] == 'SE of Change (Balanced)', 'NJ'].iloc[0]\n",
    "    # SE[DiD] = sqrt( SE[change_NJ]^2 + SE[change_PA]^2 ) assuming independence between states\n",
    "    se_did_means = np.sqrt(se_change_nj**2 + se_change_pa**2)\n",
    "\n",
    "    # Add DiD estimate and SE as summary rows\n",
    "    did_row_df = pd.DataFrame({\n",
    "        'Variable': ['DiD Estimate (Balanced Means)', 'SE of DiD Estimate (Balanced Means)'],\n",
    "        'PA': [np.nan, np.nan],\n",
    "        'NJ': [np.nan, np.nan],\n",
    "        'Difference (NJ - PA)': [did_estimate_means, se_did_means]\n",
    "    })\n",
    "\n",
    "    # Combine the main table with the summary rows\n",
    "    output_table = pd.concat([final_table_df, did_row_df], ignore_index=True)\n",
    "\n",
    "    print(\"\\n--- Summary Table: FTE Employment Before and After ---\")\n",
    "    print(output_table.round(3))\n",
    "\n",
    "    # Print sample sizes used for the balanced change calculation\n",
    "    print(\"\\nSample Sizes (Balanced Panel Used for Change Calculation):\")\n",
    "    print(balanced_stats[['state_name', 'n_balanced']].set_index('state_name'))\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping Difference-in-Differences (Means) calculation as 'data1' DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating Average Employment Changes (Table 2 Replication) ---\n",
      "\n",
      "Final Table (Means, Changes, and DiD):\n",
      "                                         PA      NJ  Difference (NJ - PA)\n",
      "Mean FTE (Before)                    23.331  20.439                -2.892\n",
      "SE (Before)                           1.351   0.508                -0.843\n",
      "Mean FTE (After)                     21.166  21.027                -0.138\n",
      "SE (After)                            0.943   0.520                -0.423\n",
      "Change in Mean FTE (Balanced)        -2.283   0.467                 2.750\n",
      "SE of Change (Balanced)               1.253   0.481                -0.772\n",
      "DiD Estimate (Balanced Means)           NaN     NaN                 2.750\n",
      "SE of DiD Estimate (Balanced Means)     NaN     NaN                 1.342\n",
      "\n",
      "Sample Sizes (Balanced Panel):\n",
      "            n_balanced\n",
      "state_name            \n",
      "PA                  75\n",
      "NJ                 309\n"
     ]
    }
   ],
   "source": [
    "if not data1.empty:\n",
    "    print(\"\\n--- Calculating Average Employment Changes (Table 2 Replication) ---\")\n",
    "\n",
    "    # Calculate means, counts, variances, and SEs for FTE before and after, grouped by state\n",
    "    summary_stats = data1.groupby('state').agg(\n",
    "        mean_before=('fte_before', 'mean'),\n",
    "        mean_after=('fte_after', 'mean'),\n",
    "        var_before=('fte_before', 'var'),\n",
    "        var_after=('fte_after', 'var'),\n",
    "        n_before=('fte_before', 'count'), # count() ignores NaNs\n",
    "        n_after=('fte_after', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate Standard Errors (SE) for the means\n",
    "    summary_stats['se_mean_before'] = np.sqrt(summary_stats['var_before'] / summary_stats['n_before'])\n",
    "    summary_stats['se_mean_after'] = np.sqrt(summary_stats['var_after'] / summary_stats['n_after'])\n",
    "\n",
    "    # Calculate change in mean FTE using all available data\n",
    "    summary_stats['change_mean_fte'] = summary_stats['mean_after'] - summary_stats['mean_before']\n",
    "\n",
    "    # Map state codes to names for clarity\n",
    "    summary_stats['state_name'] = summary_stats['state'].map({'0': 'PA', '1': 'NJ'})\n",
    "\n",
    "    # --- Balanced sample calculation (stores with non-missing FTE in both waves) ---\n",
    "    balanced_data = data1.dropna(subset=['fte_before', 'fte_after']).copy() # Crucial filter\n",
    "    balanced_stats = balanced_data.groupby('state').agg(\n",
    "        mean_before_bal=('fte_before', 'mean'),\n",
    "        mean_after_bal=('fte_after', 'mean'),\n",
    "        var_before_bal=('fte_before', 'var'),\n",
    "        var_after_bal=('fte_after', 'var'),\n",
    "        n_balanced=('sheet', 'count') # Count stores in balanced sample\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate change for the balanced sample\n",
    "    balanced_stats['change_mean_fte_balanced'] = balanced_stats['mean_after_bal'] - balanced_stats['mean_before_bal']\n",
    "\n",
    "     # Calculate covariance for SE of the change\n",
    "    cov_pa = balanced_data[balanced_data['state']=='0'][['fte_before', 'fte_after']].cov().iloc[0,1]\n",
    "    cov_nj = balanced_data[balanced_data['state']=='1'][['fte_before', 'fte_after']].cov().iloc[0,1]\n",
    "\n",
    "    # Variance and SE of the *change* within each state (balanced sample)\n",
    "    balanced_stats['var_change_bal'] = (balanced_stats['var_before_bal'] + balanced_stats['var_after_bal'] - 2 * balanced_stats['state'].map({'0': cov_pa, '1': cov_nj}))\n",
    "    balanced_stats['se_change_balanced'] = np.sqrt(balanced_stats['var_change_bal'] / balanced_stats['n_balanced'])\n",
    "\n",
    "\n",
    "    balanced_stats['state_name'] = balanced_stats['state'].map({'0': 'PA', '1': 'NJ'})\n",
    "\n",
    "    # --- Combine results into a table structure ---\n",
    "    # Select columns needed for the final table display\n",
    "    table_data = summary_stats[['state_name', 'mean_before', 'mean_after', 'se_mean_before', 'se_mean_after']].copy()\n",
    "    # Merge the balanced change and its SE\n",
    "    table_data = pd.merge(table_data, balanced_stats[['state_name', 'change_mean_fte_balanced', 'se_change_balanced', 'n_balanced']],\n",
    "                          on='state_name', how='left')\n",
    "\n",
    "    # Set state name as index for transposition\n",
    "    table_data.set_index('state_name', inplace=True)\n",
    "\n",
    "    # Transpose the table\n",
    "    transposed_table = table_data[['mean_before', 'se_mean_before', 'mean_after', 'se_mean_after', 'change_mean_fte_balanced', 'se_change_balanced']].transpose()\n",
    "\n",
    "    # Rename index for clarity\n",
    "    transposed_table.index = ['Mean FTE (Before)', 'SE (Before)', 'Mean FTE (After)', 'SE (After)', 'Change in Mean FTE (Balanced)', 'SE of Change (Balanced)']\n",
    "\n",
    "    # Calculate the NJ - PA difference column\n",
    "    transposed_table['Difference (NJ - PA)'] = transposed_table['NJ'] - transposed_table['PA']\n",
    "\n",
    "    # --- Calculate the DiD estimate and its SE (using balanced sample changes) ---\n",
    "    did_estimate_means = transposed_table.loc['Change in Mean FTE (Balanced)', 'Difference (NJ - PA)']\n",
    "\n",
    "    # SE[DiD] = sqrt( SE[change_NJ]^2 + SE[change_PA]^2 ) assuming independence of changes between states\n",
    "    se_change_pa = transposed_table.loc['SE of Change (Balanced)', 'PA']\n",
    "    se_change_nj = transposed_table.loc['SE of Change (Balanced)', 'NJ']\n",
    "    se_did_means = np.sqrt(se_change_pa**2 + se_change_nj**2)\n",
    "\n",
    "    # Add the DiD estimate and its SE as a final row for clarity\n",
    "    did_row = pd.DataFrame({\n",
    "        'PA': [np.nan], 'NJ': [np.nan], 'Difference (NJ - PA)': [did_estimate_means]\n",
    "        }, index=['DiD Estimate (Balanced Means)'])\n",
    "    se_did_row = pd.DataFrame({\n",
    "        'PA': [np.nan], 'NJ': [np.nan], 'Difference (NJ - PA)': [se_did_means]\n",
    "        }, index=['SE of DiD Estimate (Balanced Means)'])\n",
    "\n",
    "    # Append rows to the table\n",
    "    final_table = pd.concat([transposed_table, did_row, se_did_row])\n",
    "\n",
    "\n",
    "    print(\"\\nFinal Table (Means, Changes, and DiD):\")\n",
    "    # Format for better readability\n",
    "    print(final_table.round(3))\n",
    "\n",
    "    # Print sample sizes\n",
    "    print(\"\\nSample Sizes (Balanced Panel):\")\n",
    "    print(balanced_stats[['state_name', 'n_balanced']].set_index('state_name'))\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping DiD Calculation as data1 is empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
